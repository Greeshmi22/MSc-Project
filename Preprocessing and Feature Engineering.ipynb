{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d44ff5-b859-4ee4-9d78-695d77650630",
   "metadata": {},
   "source": [
    "# Extreme Rainfalls Nowcasting (UK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b78f9-ceaf-4462-8553-95cfbf511735",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Black;\">Module Code: CSMPR - MSc Project</h2>\n",
    "\n",
    "<h2 style=\"color:Black;\">Project Title: Predictive Modelling of Extreme Rainfalls</h2>\n",
    "\n",
    "<h3 style=\"color:Black;\">Student Number: <span style=\"color:green;\">32822955</span></h3>\n",
    "\n",
    "<h3 style=\"color:Black;\">Acknowledgments: Supervisor - Professor. Atta Badii, Researcher - Kieran Hunt </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b683f9-2b15-4950-96f8-ea062d89b533",
   "metadata": {},
   "source": [
    "## Setup & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf00a6a-027a-48f9-b74d-68e1426609a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdeb8db3-1107-45b2-9590-c5a629eeff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Python executable: C:\\Users\\agree\\conda_envs\\rain311\\python.exe\n",
      "‚úÖ Correct kernel detected: rain311\n",
      "‚ö° NumExpr threads set to: 16\n",
      "‚úÖ Startup cell finished. Ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Core standard libs ---\n",
    "import sys, os, pathlib, traceback, glob, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "from typing import Dict, List, Tuple\n",
    "import subprocess, shutil, site, re, time, json, math, errno, warnings, glob\n",
    "\n",
    "# --- Third-party scientific stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Environment check ---\n",
    "expected_env = \"rain311\"\n",
    "print(\"üîé Python executable:\", sys.executable)\n",
    "\n",
    "env_path = sys.executable.lower()\n",
    "if expected_env not in env_path:\n",
    "    print(f\"‚ö†Ô∏è WARNING: You are NOT running inside '{expected_env}'!\")\n",
    "    print(\"   Please change kernel in the Jupyter menu: \"\n",
    "          \"Kernel ‚Üí Change Kernel ‚Üí Python (rain311)\")\n",
    "else:\n",
    "    print(f\"‚úÖ Correct kernel detected: {expected_env}\")\n",
    "\n",
    "# --- Control NumExpr threads (performance tuning) ---\n",
    "try:\n",
    "    import numexpr as ne\n",
    "    os.environ[\"NUMEXPR_MAX_THREADS\"] = \"16\"\n",
    "    ne.set_num_threads(16)\n",
    "    print(f\"‚ö° NumExpr threads set to: {ne.nthreads}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è NumExpr is not installed. Please ensure it's in environment.yml\")\n",
    "\n",
    "print(\"‚úÖ Startup cell finished. Ready to go!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31460147-a951-4a8d-83f7-087c0fdeb0bb",
   "metadata": {},
   "source": [
    "## Setup & helpers (updated for separate inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f22144-9a46-40f2-bc9a-e81a7c1461c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7556f4-076b-413d-a586-05de4bb7ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# Setup & Helpers (clean + consistent)\n",
    "# ==============================================\n",
    "\n",
    "import os, glob, warnings\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Project paths\n",
    "# ----------------------------\n",
    "BASE = Path(\"D:/extreme_rainfalls\")\n",
    "RAW  = BASE / \"data\" / \"raw\"\n",
    "PROC = BASE / \"data\" / \"processed\"\n",
    "\n",
    "# Input directories\n",
    "ERA5_SINGLE_RAW = RAW / \"era5_single\"\n",
    "ERA5_PL_RAW     = RAW / \"era5_pressure\"\n",
    "IMERG_RAW       = RAW / \"imerg\"\n",
    "\n",
    "# Processed directories\n",
    "ERA5_SINGLE_30 = PROC / \"era5_single_30min\"\n",
    "ERA5_PL_30     = PROC / \"era5_pressure_30min\"\n",
    "\n",
    "FE_OUT = PROC / \"feature_engineered\"\n",
    "FE_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Standardised outputs\n",
    "ERA5_STD_SINGLE = PROC / \"era5_standardised\" / \"single\"\n",
    "ERA5_STD_PL     = PROC / \"era5_standardised\" / \"pressure\"\n",
    "IMERG_STD       = PROC / \"imerg_standardised\"\n",
    "\n",
    "for d in [ERA5_STD_SINGLE, ERA5_STD_PL, IMERG_STD]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# --- Finalized variables ---\n",
    "ERA5_SINGLE_VARS = [\n",
    "    \"total_precipitation\",\"convective_precipitation\",\"2m_temperature\",\n",
    "    \"10m_u_component_of_wind\",\"10m_v_component_of_wind\",\n",
    "    \"surface_pressure\",\"total_column_water_vapour\",\n",
    "    \"total_column_cloud_liquid_water\",\"boundary_layer_height\",\"mean_sea_level_pressure\"\n",
    "]\n",
    "ERA5_PL_LEVELS = [\"850\",\"700\",\"500\",\"300\"]\n",
    "ERA5_PL_VARS = [\"specific_humidity\",\"u_component_of_wind\",\"v_component_of_wind\",\n",
    "                \"geopotential\",\"relative_humidity\",\"temperature\"]\n",
    "\n",
    "IMERG_VARS = [\n",
    "    \"precipitationCal\",\"precipitationUncal\",\"precipitationQualityIndex\",\n",
    "    \"randomError\",\"probabilityLiquidPrecipitation\"\n",
    "]\n",
    "\n",
    "ACCUM_VARS = {\"total_precipitation\", \"convective_precipitation\"} # hourly totals (m)\n",
    "\n",
    "# Years in scope\n",
    "YEARS       = list(range(2015, 2020))      # 2015‚Äì2025 inclusive\n",
    "TRAIN_YEARS = list(range(2015, 2022))      # 2015‚Äì2021\n",
    "VAL_YEARS   = [2022, 2023]\n",
    "TEST_YEARS  = [2024, 2025]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb97086d-d607-43a7-a93b-fd5d5d151b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Normalisation helper\n",
    "# ----------------------------\n",
    "def normalize_dataset(ds: xr.Dataset, path: Path = None, do_transpose: bool = True) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Clean and standardize a dataset:\n",
    "    - Ensures 'time' coordinate exists (renames valid_time / forecast_time if needed)\n",
    "    - Drops ERA5 expver dimension if present\n",
    "    - Standardizes lat/lon naming and ascending order\n",
    "    - Normalizes ERA5 short variable names (tp ‚Üí total_precipitation, etc.)\n",
    "    - Optionally transposes to a consistent dimension order\n",
    "    \"\"\"\n",
    "\n",
    "    # Fix time coordinate\n",
    "    if \"time\" not in ds.coords:\n",
    "        for candidate in [\"valid_time\", \"forecast_time\", \"forecast_reference_time\"]:\n",
    "            if candidate in ds.coords:\n",
    "                ds = ds.rename({candidate: \"time\"})\n",
    "                if path: print(f\"‚ö†Ô∏è Renamed '{candidate}' ‚Üí 'time' in {path}\")\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"No 'time' coordinate found. Got coords: {list(ds.coords)}\")\n",
    "\n",
    "    # Drop expver if present\n",
    "    if \"expver\" in ds.dims:\n",
    "        ds = ds.isel(expver=-1, drop=True)\n",
    "        if path: print(f\"‚ÑπÔ∏è Dropped 'expver' in {path}\")\n",
    "\n",
    "    # Standardize lat/lon coords\n",
    "    rename_map = {}\n",
    "    if \"latitude\" in ds.coords:  rename_map[\"latitude\"] = \"lat\"\n",
    "    if \"y\" in ds.coords:         rename_map[\"y\"] = \"lat\"\n",
    "    if \"longitude\" in ds.coords: rename_map[\"longitude\"] = \"lon\"\n",
    "    if \"x\" in ds.coords:         rename_map[\"x\"] = \"lon\"\n",
    "    if rename_map:\n",
    "        ds = ds.rename(rename_map)\n",
    "\n",
    "    # Ensure ascending\n",
    "    if \"lat\" in ds.coords and ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.reindex(lat=ds.lat[::-1])\n",
    "    if \"lon\" in ds.coords and ds.lon[0] > ds.lon[-1]:\n",
    "        ds = ds.reindex(lon=ds.lon[::-1])\n",
    "\n",
    "    # Normalize ERA5 variable names\n",
    "    var_map = {\n",
    "        # ERA5 single-level\n",
    "        \"tp\": \"total_precipitation\",\n",
    "        \"cp\": \"convective_precipitation\",\n",
    "        \"t2m\": \"2m_temperature\",\n",
    "        \"u10\": \"10m_u_component_of_wind\",\n",
    "        \"v10\": \"10m_v_component_of_wind\",\n",
    "        \"sp\": \"surface_pressure\",\n",
    "        \"tcwv\": \"total_column_water_vapour\",\n",
    "        \"tclw\": \"total_column_cloud_liquid_water\",\n",
    "        \"blh\": \"boundary_layer_height\",\n",
    "        \"msl\": \"mean_sea_level_pressure\",\n",
    "        # ERA5 pressure-level\n",
    "        \"q\": \"specific_humidity\",\n",
    "        \"u\": \"u_component_of_wind\",\n",
    "        \"v\": \"v_component_of_wind\",\n",
    "        \"z\": \"geopotential\",\n",
    "        \"r\": \"relative_humidity\",\n",
    "        \"t\": \"temperature\",\n",
    "    }\n",
    "    rename_dict = {k: v for k, v in var_map.items() if k in ds.data_vars}\n",
    "    if rename_dict:\n",
    "        ds = ds.rename(rename_dict)\n",
    "        if path: print(f\"‚ÑπÔ∏è Renamed variables in {path}: {rename_dict}\")\n",
    "\n",
    "    # Transpose for consistency\n",
    "    if do_transpose:\n",
    "        dims = list(ds.dims)\n",
    "        if set([\"time\", \"pressure_level\", \"lat\", \"lon\"]).issubset(dims):\n",
    "            ds = ds.transpose(\"time\", \"pressure_level\", \"lat\", \"lon\")\n",
    "        elif set([\"time\", \"lat\", \"lon\"]).issubset(dims):\n",
    "            ds = ds.transpose(\"time\", \"lat\", \"lon\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "# ----------------------------\n",
    "# Utility helpers\n",
    "# ----------------------------\n",
    "def find_any(path: Path, pattern: str):\n",
    "    \"\"\"Return first matching file, or None if none exist.\"\"\"\n",
    "    hits = sorted(glob.glob(str(path / pattern)))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def print_ds_summary(ds: xr.Dataset, name: str = \"Dataset\"):\n",
    "    \"\"\"Print compact summary of variables, dimensions, and units.\"\"\"\n",
    "    print(f\"\\nüìä Summary for {name}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Dimensions: {dict(ds.dims)}\")\n",
    "    print(f\"Coordinates: {list(ds.coords)}\")\n",
    "    print(\"-\" * 60)\n",
    "    for var in ds.data_vars:\n",
    "        da = ds[var]\n",
    "        shape = \" √ó \".join([f\"{d}={da.sizes[d]}\" for d in da.dims])\n",
    "        units = da.attrs.get(\"units\", \"‚Äî\")\n",
    "        print(f\"{var:30s} | {shape:40s} | units: {units}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper to save engineered feature\n",
    "# -----------------------------\n",
    "def save_feature(ds: xr.Dataset, name: str):\n",
    "    out_fp = FE_OUT / f\"{name}_2015_2025.nc\"\n",
    "    ds.to_netcdf(out_fp, mode=\"w\")\n",
    "    print(f\"‚úÖ Saved {name} ‚Üí {out_fp}\")\n",
    "    return out_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eaa685-fde5-42c9-9067-dd586003077c",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4e46d-ea16-47fc-81b1-1d0c179771eb",
   "metadata": {},
   "source": [
    "### 1. ERA5 ‚Üí 30-minute timestamps (to match IMERG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1eea658-e64c-4d0f-9667-4b97f0c18ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: D:\\extreme_rainfalls\\data\\raw\\era5_single\\2015\\era5_single_2m_temperature_201501.nc\n",
      "\n",
      "Coordinates: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "\n",
      "Dimensions: FrozenMappingWarningOnValuesAccess({'valid_time': 744, 'latitude': 53, 'longitude': 67})\n",
      "\n",
      "Variables: ['t2m']\n",
      "\n",
      "Time preview:\n",
      " <xarray.DataArray 'valid_time' (valid_time: 744)> Size: 6kB\n",
      "array(['2015-01-01T00:00:00.000000000', '2015-01-01T01:00:00.000000000',\n",
      "       '2015-01-01T02:00:00.000000000', ..., '2015-01-31T21:00:00.000000000',\n",
      "       '2015-01-31T22:00:00.000000000', '2015-01-31T23:00:00.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "    number      int64 8B ...\n",
      "  * valid_time  (valid_time) datetime64[ns] 6kB 2015-01-01 ... 2015-01-31T23:...\n",
      "    expver      (valid_time) <U4 12kB ...\n",
      "Attributes:\n",
      "    long_name:      time\n",
      "    standard_name:  time\n"
     ]
    }
   ],
   "source": [
    "def _ensure_sorted(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Consistent coordinate order (time asc, latitude asc, longitude asc if needed).\"\"\"\n",
    "    \n",
    "    # Sort by time if present\n",
    "    if \"time\" in ds.coords:\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "    # Handle latitude naming variations\n",
    "    lat_name = None\n",
    "    for candidate in [\"latitude\", \"lat\", \"y\"]:\n",
    "        if candidate in ds.coords:\n",
    "            lat_name = candidate\n",
    "            break\n",
    "\n",
    "    if lat_name:\n",
    "        if ds[lat_name][0] > ds[lat_name][-1]:\n",
    "            ds = ds.reindex({lat_name: ds[lat_name][::-1]})\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No latitude/lat/y coordinate found; skipping lat reindex.\")\n",
    "\n",
    "    # Handle longitude naming variations\n",
    "    lon_name = None\n",
    "    for candidate in [\"longitude\", \"lon\", \"x\"]:\n",
    "        if candidate in ds.coords:\n",
    "            lon_name = candidate\n",
    "            break\n",
    "\n",
    "    if lon_name:\n",
    "        # Optional: ensure monotonic increasing (common for regridding/merging)\n",
    "        if ds[lon_name][0] > ds[lon_name][-1]:\n",
    "            ds = ds.reindex({lon_name: ds[lon_name][::-1]})\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No longitude/lon/x coordinate found; skipping lon reindex.\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Pick one example ERA5 single-level file from your raw folder\n",
    "sample_files = glob.glob(str(ERA5_SINGLE_RAW / \"2015\" / \"*.nc\"))\n",
    "if sample_files:\n",
    "    fp = sample_files[0]\n",
    "    print(\"Inspecting file:\", fp)\n",
    "    ds = xr.open_dataset(fp)\n",
    "    print(\"\\nCoordinates:\", list(ds.coords))\n",
    "    print(\"\\nDimensions:\", ds.dims)\n",
    "    print(\"\\nVariables:\", list(ds.data_vars))\n",
    "    print(\"\\nTime preview:\\n\", ds.coords.get(\"time\", ds.coords.get(\"valid_time\", \"No time coord found\")))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No sample files found in\", ERA5_SINGLE_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef20072-f99b-48b0-a85d-5de4384dcd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_sorted(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Consistent coordinate order (time asc, latitude asc).\"\"\"\n",
    "    if \"time\" in ds.coords:\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "    # Handle latitude naming variations\n",
    "    lat_name = None\n",
    "    for candidate in [\"latitude\", \"lat\", \"y\"]:\n",
    "        if candidate in ds.coords:\n",
    "            lat_name = candidate\n",
    "            break\n",
    "\n",
    "    if lat_name:\n",
    "        if ds[lat_name][0] > ds[lat_name][-1]:\n",
    "            ds = ds.reindex({lat_name: ds[lat_name][::-1]})\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No latitude/lat/y coordinate found; skipping lat reindex.\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _drop_expver(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"expver\" in ds.dims:\n",
    "        ds = ds.isel(expver=-1, drop=True)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e1aeab-901e-4fab-8563-f098e32389f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inst_to_30min(da: xr.DataArray) -> xr.DataArray:\n",
    "    if \"time\" not in da.coords:\n",
    "        raise ValueError(f\"{da.name} has no 'time' coordinate\")\n",
    "    out = da.resample(time=\"30min\").interpolate(\"linear\")\n",
    "    out.attrs.update(da.attrs)\n",
    "    return out\n",
    "\n",
    "def _accum_to_mm_per_30min(da: xr.DataArray) -> xr.DataArray:\n",
    "    if \"time\" not in da.coords:\n",
    "        raise ValueError(f\"{da.name} has no 'time' coordinate\")\n",
    "    mm_per_hour = da * 1000.0\n",
    "    mm_per_30   = mm_per_hour.resample(time=\"30min\").pad() / 2.0\n",
    "    mm_per_30.attrs[\"units\"] = \"mm/30min\"\n",
    "    return mm_per_30\n",
    "\n",
    "# ======================================================\n",
    "# Step-1: Open ERA5 single-level (raw) ‚Üí Resample to 30min\n",
    "# ======================================================\n",
    "\n",
    "def open_era5_single_month(year: int, month: int) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Open ERA5 single-level data for a given year and month.\n",
    "    Applies normalization (time, lat/lon, variable names).\n",
    "    \"\"\"\n",
    "    folder = ERA5_SINGLE_RAW / str(year) / f\"{month:02d}\"\n",
    "    files = sorted(glob.glob(str(folder / \"*.nc\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No ERA5 single-level files found for {year}-{month:02d} in {folder}\")\n",
    "\n",
    "    ds = xr.open_mfdataset(files, combine=\"by_coords\", decode_times=True)\n",
    "    return normalize_dataset(ds, path=folder)\n",
    "\n",
    "\n",
    "\n",
    "def resample_era5_to_30min(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Resample ERA5 (hourly) data to 30-minutes.\"\"\"\n",
    "    # Detect accumulated vs instantaneous\n",
    "    accum_vars = {\"total_precipitation\", \"convective_precipitation\"}\n",
    "    vars_in = list(ds.data_vars)\n",
    "    inst_vars = [v for v in vars_in if v not in accum_vars]\n",
    "    acc_vars  = [v for v in vars_in if v in accum_vars]\n",
    "\n",
    "    # Instantaneous ‚Üí interpolate\n",
    "    out = {}\n",
    "    if inst_vars:\n",
    "        out_inst = ds[inst_vars].resample(time=\"30min\").interpolate(\"linear\")\n",
    "        out.update(out_inst)\n",
    "\n",
    "    # Accumulated ‚Üí difference\n",
    "    if acc_vars:\n",
    "        out_acc = ds[acc_vars].resample(time=\"30min\").sum()\n",
    "        out.update(out_acc)\n",
    "\n",
    "    return xr.Dataset(out, coords={\"time\": ds.time.resample(time=\"30min\").asfreq()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9d95a4-0d77-40ec-a2ee-55841cafb636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Step-2: Open ERA5 pressure-level (raw) ‚Üí Resample to 30min\n",
    "# ======================================================\n",
    "\n",
    "def open_era5_pl_month(year: int, month: int) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Open ERA5 pressure-level data for a given year and month.\n",
    "    Applies normalization (time, lat/lon, variable names).\n",
    "    \"\"\"\n",
    "    folder = ERA5_PL_RAW / str(year) / f\"{month:02d}\"\n",
    "    files = sorted(glob.glob(str(folder / \"*.nc\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No ERA5 pressure-level files found for {year}-{month:02d} in {folder}\")\n",
    "\n",
    "    ds = xr.open_mfdataset(files, combine=\"by_coords\", decode_times=True)\n",
    "    return normalize_dataset(ds, path=folder)\n",
    "\n",
    "\n",
    "def resample_era5pl_to_30min(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Resample ERA5 pressure-level (hourly) data to 30-minutes.\"\"\"\n",
    "    out = ds.resample(time=\"30min\").interpolate(\"linear\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92396185-cfa4-4792-957c-221c8420a15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Step 1 skipped (already processed)\n"
     ]
    }
   ],
   "source": [
    "# Build list of all (year, month) pairs\n",
    "\n",
    "\n",
    "RUN_STEP_1 = False   # change to True only if you want to rerun\n",
    "\n",
    "if RUN_STEP_1:\n",
    "    all_months = [(y, m) for y in YEARS for m in range(1, 13)]\n",
    "    for year, month in tqdm(all_months, desc=\"Processing ERA5 ‚Üí 30min\", unit=\"month\", ncols=100):\n",
    "        try:\n",
    "            ds_s = open_era5_single_month(year, month)\n",
    "            ds_s_30 = resample_era5_to_30min(ds_s)\n",
    "            tmp_fp = ERA5_SINGLE_30 / f\"era5_single_30min_{year}{month:02d}.tmp.nc\"\n",
    "            ds_s_30.to_netcdf(tmp_fp, mode=\"w\")\n",
    "            os.replace(tmp_fp, ERA5_SINGLE_30 / f\"era5_single_30min_{year}{month:02d}.nc\")\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            ds_pl = open_era5_pl_month(year, month)\n",
    "            ds_pl_30 = resample_era5_to_30min(ds_pl)\n",
    "            tmp_fp = ERA5_PL_30 / f\"era5_pl_30min_{year}{month:02d}.tmp.nc\"\n",
    "            ds_pl_30.to_netcdf(tmp_fp, mode=\"w\")\n",
    "            os.replace(tmp_fp, ERA5_PL_30 / f\"era5_pl_30min_{year}{month:02d}.nc\")\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    print(\"‚úÖ Step 1 done: ERA5 ‚Üí 30min resampling complete\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Step 1 skipped (already processed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdb9588-08fa-4d76-b769-9ce38aef439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying h5py + HDF5 library versions...\n",
      "h5py version: 3.14.0\n",
      "HDF5 library version: 1.14.6\n",
      "‚úÖ HDF5 library is consistent with h5py build.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# Step Verify h5py & HDF5 versions before IMERG usage\n",
    "# =======================================================\n",
    "import h5py, warnings\n",
    "\n",
    "print(\"üîç Verifying h5py + HDF5 library versions...\")\n",
    "\n",
    "try:\n",
    "    print(\"h5py version:\", h5py.__version__)\n",
    "    print(\"HDF5 library version:\", h5py.version.hdf5_version)\n",
    "    \n",
    "    if h5py.version.hdf5_version.startswith(\"1.14\"):\n",
    "        print(\"‚úÖ HDF5 library is consistent with h5py build.\")\n",
    "    else:\n",
    "        warnings.warn(\"‚ö†Ô∏è Potential mismatch between h5py and HDF5 runtime libraries!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to check h5py/HDF5 versions:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b252e4-a2eb-47dc-ac16-7220b0a0a781",
   "metadata": {},
   "source": [
    "## 2. Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789c84f1-04f3-4da3-bf1c-9d26d4d967da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped (already standardised)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Step-2: Standardisation of datasets\n",
    "# ==========================================\n",
    "def normalize_dataset(ds: xr.Dataset, path: Path = None, do_transpose: bool = True) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Clean and standardize a dataset:\n",
    "    - Ensures 'time' coordinate exists (handles IMERG special case with startTime/endTime or attrs)\n",
    "    - Drops ERA5 expver dimension if present\n",
    "    - Standardizes lat/lon naming and ascending order\n",
    "    - Normalizes ERA5 short variable names\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Fix time\n",
    "    # ----------------------------\n",
    "    if \"time\" not in ds.coords:\n",
    "        if \"valid_time\" in ds.coords:\n",
    "            ds = ds.rename({\"valid_time\": \"time\"})\n",
    "        elif \"forecast_time\" in ds.coords:\n",
    "            ds = ds.rename({\"forecast_time\": \"time\"})\n",
    "        elif \"forecast_reference_time\" in ds.coords:\n",
    "            ds = ds.rename({\"forecast_reference_time\": \"time\"})\n",
    "        elif \"time\" in ds.variables:\n",
    "            # Promote variable to coordinate\n",
    "            ds = ds.assign_coords(time=ds[\"time\"].values)\n",
    "        elif \"startTime\" in ds.variables and \"endTime\" in ds.variables:\n",
    "            # IMERG: midpoint of startTime and endTime\n",
    "            start = np.array(ds[\"startTime\"].values, dtype=\"datetime64[ns]\")\n",
    "            end   = np.array(ds[\"endTime\"].values, dtype=\"datetime64[ns]\")\n",
    "            midpoint = start + (end - start) // 2\n",
    "            ds = ds.expand_dims({\"time\": [midpoint]})\n",
    "            if path: print(f\"‚è±Ô∏è Created midpoint time for IMERG {path}\")\n",
    "        elif \"time_coverage_start\" in ds.attrs and \"time_coverage_end\" in ds.attrs:\n",
    "            # Fallback to global attributes\n",
    "            t0 = np.datetime64(ds.attrs[\"time_coverage_start\"])\n",
    "            t1 = np.datetime64(ds.attrs[\"time_coverage_end\"])\n",
    "            midpoint = t0 + (t1 - t0) // 2\n",
    "            ds = ds.expand_dims({\"time\": [midpoint]})\n",
    "            if path: print(f\"‚è±Ô∏è Created midpoint time from attrs for IMERG {path}\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No usable 'time' found in {path}. \"\n",
    "                f\"Coords: {list(ds.coords)}, Vars: {list(ds.variables)}, Attrs: {list(ds.attrs.keys())}\"\n",
    "            )\n",
    "\n",
    "    # Drop expver if present\n",
    "    if \"expver\" in ds.dims:\n",
    "        ds = ds.isel(expver=-1, drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Standardize lat/lon coords\n",
    "    # ----------------------------\n",
    "    rename_map = {}\n",
    "    if \"latitude\" in ds.coords:  rename_map[\"latitude\"] = \"lat\"\n",
    "    if \"y\" in ds.coords:         rename_map[\"y\"] = \"lat\"\n",
    "    if \"longitude\" in ds.coords: rename_map[\"longitude\"] = \"lon\"\n",
    "    if \"x\" in ds.coords:         rename_map[\"x\"] = \"lon\"\n",
    "    if rename_map:\n",
    "        ds = ds.rename(rename_map)\n",
    "\n",
    "    # Ensure ascending\n",
    "    if \"lat\" in ds.coords and ds.lat[0] > ds.lat[-1]:\n",
    "        ds = ds.reindex(lat=ds.lat[::-1])\n",
    "    if \"lon\" in ds.coords and ds.lon[0] > ds.lon[-1]:\n",
    "        ds = ds.reindex(lon=ds.lon[::-1])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Normalize ERA5 variable names (skip IMERG vars, they‚Äôre already descriptive)\n",
    "    # ----------------------------\n",
    "    var_map = {\n",
    "        \"tp\": \"total_precipitation\",\n",
    "        \"cp\": \"convective_precipitation\",\n",
    "        \"t2m\": \"2m_temperature\",\n",
    "        \"u10\": \"10m_u_component_of_wind\",\n",
    "        \"v10\": \"10m_v_component_of_wind\",\n",
    "        \"sp\": \"surface_pressure\",\n",
    "        \"tcwv\": \"total_column_water_vapour\",\n",
    "        \"tclw\": \"total_column_cloud_liquid_water\",\n",
    "        \"blh\": \"boundary_layer_height\",\n",
    "        \"msl\": \"mean_sea_level_pressure\",\n",
    "        \"q\": \"specific_humidity\",\n",
    "        \"u\": \"u_component_of_wind\",\n",
    "        \"v\": \"v_component_of_wind\",\n",
    "        \"z\": \"geopotential\",\n",
    "        \"r\": \"relative_humidity\",\n",
    "        \"t\": \"temperature\",\n",
    "    }\n",
    "    rename_dict = {k: v for k, v in var_map.items() if k in ds.data_vars}\n",
    "    if rename_dict:\n",
    "        ds = ds.rename(rename_dict)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Safe transpose\n",
    "    # ----------------------------\n",
    "    if do_transpose:\n",
    "        dims = list(ds.dims)\n",
    "        if set([\"time\", \"pressure_level\", \"lat\", \"lon\"]).issubset(dims):\n",
    "            ds = ds.transpose(\"time\", \"pressure_level\", \"lat\", \"lon\")\n",
    "        elif set([\"time\", \"lat\", \"lon\"]).issubset(dims):\n",
    "            ds = ds.transpose(\"time\", \"lat\", \"lon\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "# --- Apply standardisation to all datasets ---\n",
    "\n",
    "RUN_STEP_1 = False   # change to True only if you want to rerun\n",
    "\n",
    "if RUN_STEP_1:\n",
    "    all_months = [(y, m) for y in YEARS for m in range(1, 13)]\n",
    "    for year in tqdm(YEARS, desc=\"Step-2 Standardising\"):\n",
    "        # ERA5 Single\n",
    "        for f in sorted((ERA5_SINGLE_30).glob(f\"*{year}*.nc\")):\n",
    "            out = ERA5_STD_SINGLE / f.name.replace(\"30min\", \"std\")\n",
    "            if out.exists(): continue\n",
    "            ds = xr.open_dataset(f, chunks={})\n",
    "            ds = normalize_dataset(ds, f)\n",
    "            ds.to_netcdf(out, engine=\"netcdf4\")\n",
    "            ds.close()\n",
    "    \n",
    "        # ERA5 Pressure\n",
    "        for f in sorted((ERA5_PL_30).glob(f\"*{year}*.nc\")):\n",
    "            out = ERA5_STD_PL / f.name.replace(\"30min\", \"std\")\n",
    "            if out.exists(): continue\n",
    "            ds = xr.open_dataset(f, chunks={})\n",
    "            ds = normalize_dataset(ds, f)\n",
    "            ds.to_netcdf(out, engine=\"netcdf4\")\n",
    "            ds.close()\n",
    "    \n",
    "        ## IMERG        \n",
    "        files = sorted((IMERG_RAW / str(year)).rglob(\"*.HDF5\")) + \\\n",
    "                sorted((IMERG_RAW / str(year)).rglob(\"*.nc4\"))\n",
    "        \n",
    "        for f in tqdm(files, desc=f\"IMERG Standardising {year}\"):\n",
    "            out = IMERG_STD / f.name.replace(\".HDF5\", \"_std.nc\").replace(\".nc4\", \"_std.nc\")\n",
    "            if out.exists():\n",
    "                print(f\"‚ö†Ô∏è Skipping existing {out}\")\n",
    "                continue\n",
    "        \n",
    "            ds = None\n",
    "            try:\n",
    "                # --- Try opening with xarray first ---\n",
    "                for engine in [\"netcdf4\", \"h5netcdf\"]:\n",
    "                    try:\n",
    "                        with xr.open_dataset(f, engine=engine, chunks={\"time\": 100}) as ds_tmp:\n",
    "                            ds = ds_tmp.load()  # load into memory\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Engine {engine} failed for {f.name}: {e}\")\n",
    "                        ds = None\n",
    "        \n",
    "                # --- If still no dataset, fallback to h5py ---\n",
    "                if ds is None or len(ds.data_vars) == 0:\n",
    "                    import h5py\n",
    "                    with h5py.File(f, \"r\") as h5f:\n",
    "                        # IMERG main variable usually 'precipitationCal'\n",
    "                        var_name = \"precipitationCal\" if \"precipitationCal\" in h5f.keys() else list(h5f.keys())[0]\n",
    "                        arr = np.array(h5f[var_name])\n",
    "                        lat = np.array(h5f[\"lat\"])\n",
    "                        lon = np.array(h5f[\"lon\"])\n",
    "                        ds = xr.Dataset(\n",
    "                            {\"precipitation\": ((\"lat\", \"lon\"), arr)},\n",
    "                            coords={\"lat\": lat, \"lon\": lon}\n",
    "                        )\n",
    "        \n",
    "                # --- Ensure time coordinate exists ---\n",
    "                if \"time\" not in ds.coords:\n",
    "                    m = re.search(r\"(\\d{8})-S(\\d{6})-E(\\d{6})\", f.name)\n",
    "                    if m:\n",
    "                        date_str, s_time, e_time = m.groups()\n",
    "                        start = pd.to_datetime(date_str + s_time, format=\"%Y%m%d%H%M%S\")\n",
    "                        end   = pd.to_datetime(date_str + e_time, format=\"%Y%m%d%H%M%S\")\n",
    "                        midpoint = np.datetime64(start + (end - start) / 2)\n",
    "                        ds = ds.expand_dims({\"time\": [midpoint]})\n",
    "                        print(f\"‚è±Ô∏è Parsed time from filename for {f.name}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Could not parse time from {f.name}, skipping\")\n",
    "                        continue\n",
    "        \n",
    "                # --- Normalise coords/lat/lon ---\n",
    "                ds = normalize_dataset(ds, f, do_transpose=True)\n",
    "        \n",
    "                # --- Save compressed & float32 ---\n",
    "                tmp_out = out.with_suffix(\".tmp.nc\")\n",
    "                ds.astype(\"float32\").to_netcdf(\n",
    "                    tmp_out,\n",
    "                    engine=\"netcdf4\",\n",
    "                    encoding={v: {\"zlib\": True, \"complevel\": 4} for v in ds.data_vars}\n",
    "                )\n",
    "                os.replace(tmp_out, out)  # safe overwrite\n",
    "                ds.close()\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Could not standardise {f.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "    print(\" Standardisation of ERA5 & IMERG datasets completed\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipped (already standardised)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9dd07-5e6c-451c-b555-8535a3930a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2c504-cb79-46bf-ac38-0ad44fbabd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37bc46fe-eb2c-4fbb-a88b-c7165bd22412",
   "metadata": {},
   "source": [
    "## 3: Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e6110c-d628-48df-9abe-66c4b5e05906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Step-3: Data Quality Checks (lazy/dask safe)\n",
    "# =====================================\n",
    "import dask\n",
    "import dask.array as da\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_duplicates_and_monotonic(ds, name=\"Dataset\"):\n",
    "    \"\"\"Check duplicates + monotonicity of time/lat/lon without loading full data.\"\"\"\n",
    "    print(f\"\\nüîç Checking duplicates & monotonicity for {name} ...\")\n",
    "    for coord in [\"time\", \"lat\", \"lon\"]:\n",
    "        if coord in ds.coords:\n",
    "            idx = ds.indexes[coord]\n",
    "            if not idx.is_monotonic_increasing:\n",
    "                print(f\"‚ö†Ô∏è {coord} not monotonic ‚Üí sorting\")\n",
    "                ds = ds.sortby(coord)\n",
    "            if idx.has_duplicates:\n",
    "                print(f\"‚ö†Ô∏è {coord} has duplicates ‚Üí dropping\")\n",
    "                ds = ds.sel({coord: ~idx.duplicated()})\n",
    "    return ds\n",
    "\n",
    "\n",
    "def check_physical_ranges(ds, var_ranges: dict, name=\"Dataset\"):\n",
    "    \"\"\"Clamp variable values to known physical ranges (lazy, chunked).\"\"\"\n",
    "    print(f\"\\nüîç Checking/clamping ranges for {name} ...\")\n",
    "    for var, (vmin, vmax) in var_ranges.items():\n",
    "        if var in ds.data_vars:\n",
    "            da_var = ds[var]\n",
    "            # Clamp lazily\n",
    "            ds[var] = da.clip(da_var, vmin, vmax)\n",
    "            print(f\"  {var}: clamped to [{vmin}, {vmax}]\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def check_missing_data(ds, name=\"Dataset\"):\n",
    "    \"\"\"Check NaNs without loading all data.\"\"\"\n",
    "    print(f\"\\nüîç Checking missing data for {name} ...\")\n",
    "    for var in ds.data_vars:\n",
    "        try:\n",
    "            has_nan = ds[var].isnull().any().compute()\n",
    "            if has_nan:\n",
    "                print(f\"‚ö†Ô∏è {var}: contains NaNs\")\n",
    "            else:\n",
    "                print(f\"  {var}: no NaNs\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {var}: check failed ({e})\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example variable ranges\n",
    "# ----------------------------\n",
    "PHYSICAL_RANGES = {\n",
    "    \"total_precipitation\": (0, 0.5),      # m over 30 min (~1000 mm/h upper cap)\n",
    "    \"convective_precipitation\": (0, 0.5),\n",
    "    \"2m_temperature\": (180, 330),         # K\n",
    "    \"10m_u_component_of_wind\": (-100, 100),\n",
    "    \"10m_v_component_of_wind\": (-100, 100),\n",
    "    \"surface_pressure\": (5e4, 1.1e5),     # Pa\n",
    "    \"specific_humidity\": (0, 0.1),\n",
    "    \"relative_humidity\": (0, 100),\n",
    "    \"temperature\": (180, 330),            # K\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run checks for each dataset\n",
    "# ----------------------------\n",
    "def run_quality_checks(input_dir, output_dir, label):\n",
    "    files = sorted(glob.glob(str(input_dir / \"*.nc\")))\n",
    "    for f in tqdm(files, desc=f\"Step-3 Quality Checks: {label}\"):\n",
    "        ds = xr.open_dataset(f, chunks={\"time\": 100})   # lazy open\n",
    "        ds = normalize_dataset(ds, f)\n",
    "\n",
    "        ds = check_duplicates_and_monotonic(ds, name=label)\n",
    "        ds = check_physical_ranges(ds, PHYSICAL_RANGES, name=label)\n",
    "        ds = check_missing_data(ds, name=label)\n",
    "\n",
    "        # Save cleaned copy (NetCDF, compressed, float32)\n",
    "        out = output_dir / Path(f).name\n",
    "        if not out.exists():  # safeguard: skip if already processed\n",
    "            ds.astype(\"float32\").to_netcdf(out, engine=\"h5netcdf\", \n",
    "                                           encoding={v: {\"zlib\": True, \"complevel\": 4} for v in ds.data_vars})\n",
    "        ds.close()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Apply to ERA5 & IMERG\n",
    "# ----------------------------\n",
    "# ERA5 single\n",
    "# run_quality_checks(ERA5_SINGLE_30, ERA5_STD_SINGLE, \"ERA5-Single\") - commented as the process is completed (uncommnet to rerun)\n",
    "\n",
    "# ERA5 pressure-level\n",
    "#run_quality_checks(ERA5_PL_30, ERA5_STD_PL, \"ERA5-Pressure\") - commented as the process is completed (uncommnet to rerun)\n",
    "\n",
    "# IMERG\n",
    "# run_quality_checks(IMERG_RAW, IMERG_STD, \"IMERG\") - commented as the process is completed (uncommnet to rerun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55db4bd-c0f4-4900-9acd-6b8f2bd7c2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb477384-1cc0-4344-92ea-20ea7bc07a58",
   "metadata": {},
   "source": [
    "## 4. Splits for Training/Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49d20e-76de-4b6e-b6eb-36411737c613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea90a74b-3186-438b-9d77-43e9bd9494bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_on_imerg_30min_201501.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201501.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201502.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 4it [00:00, 35.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201503.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201504.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201505.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 8it [00:00, 32.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201506.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201507.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201508.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201509.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201510.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 12it [00:00, 33.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201511.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201512.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 16it [00:00, 32.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201601.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201602.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201603.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201604.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201605.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 20it [00:00, 31.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201606.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201607.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 24it [00:00, 31.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201608.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201609.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201610.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201611.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201612.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201701.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 31it [00:01, 28.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201702.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201703.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201704.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201705.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201706.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201707.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201708.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 39it [00:01, 29.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201709.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201710.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201711.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201712.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201801.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201802.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201803.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 45it [00:01, 29.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201804.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201805.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201806.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201807.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201808.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201809.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 51it [00:01, 28.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201810.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201811.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201812.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201901.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201902.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201903.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 58it [00:01, 29.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201904.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201905.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201906.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201907.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201908.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201909.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201910.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 64it [00:02, 29.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201911.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_201912.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202001.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202002.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202003.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202004.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202005.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 71it [00:02, 29.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202006.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202007.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202008.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202009.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202010.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202011.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202012.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 79it [00:02, 29.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202101.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202102.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202103.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202104.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202105.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202106.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202107.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 85it [00:02, 29.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202108.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202109.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202110.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202111.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202112.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202201.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 91it [00:03, 29.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202202.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202203.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202204.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202205.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202206.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202207.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 97it [00:03, 28.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202208.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202209.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202210.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202211.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202212.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202301.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 103it [00:03, 25.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202302.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202303.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202304.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202305.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202306.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 109it [00:03, 27.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202307.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202308.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202309.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202310.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202311.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202312.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 115it [00:03, 27.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202401.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202402.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202403.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202404.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202405.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202406.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 118it [00:04, 26.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202407.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202408.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202409.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202410.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202411.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202412.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 125it [00:04, 27.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202501.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202502.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202503.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202504.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202505.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202506.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 131it [00:04, 26.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202507.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_std_202508.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201501.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201502.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201503.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 137it [00:04, 27.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201504.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201505.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201506.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201507.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201508.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201509.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 143it [00:04, 27.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201510.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201511.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201512.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201601.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201602.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201603.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 150it [00:05, 28.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201604.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201605.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201606.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201607.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201608.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201609.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201610.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 157it [00:05, 28.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201611.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201612.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201701.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201702.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201703.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201704.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201705.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 163it [00:05, 27.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201706.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201707.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201708.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201709.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201710.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201711.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 170it [00:05, 27.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201712.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201801.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201802.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201803.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201804.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201805.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 176it [00:06, 27.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201806.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201807.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201808.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201809.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201810.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201811.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 183it [00:06, 28.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201812.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201901.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201902.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201903.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201904.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201905.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201906.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step-5 Splits single: 186it [00:06, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201907.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201908.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "‚ö†Ô∏è Skipping D:\\extreme_rainfalls\\data\\processed\\era5_standardised\\single\\era5_single_30min_201909.nc: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m tqdm(folder.glob(\u001b[33m\"\u001b[39m\u001b[33m*.nc\u001b[39m\u001b[33m\"\u001b[39m), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep-5 Splits \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         ds = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         split_label = label_split(ds.time)\n\u001b[32m     23\u001b[39m         split_records.append({\n\u001b[32m     24\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: f.name,\n\u001b[32m     25\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: folder.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m: split_label,\n\u001b[32m     30\u001b[39m         })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\backends\\api.py:668\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    665\u001b[39m     kwargs.update(backend_kwargs)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     engine = \u001b[43mplugins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    671\u001b[39m     from_array_kwargs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\backends\\plugins.py:147\u001b[39m, in \u001b[36mguess_engine\u001b[39m\u001b[34m(store_spec)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine, backend \u001b[38;5;129;01min\u001b[39;00m engines.items():\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguess_can_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_spec\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m engine\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\backends\\netCDF4_.py:633\u001b[39m, in \u001b[36mNetCDF4BackendEntrypoint.guess_can_open\u001b[39m\u001b[34m(self, filename_or_obj)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_remote_uri(filename_or_obj):\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m magic_number = \u001b[43mtry_read_magic_number_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    635\u001b[39m     \u001b[38;5;66;03m# netcdf 3 or HDF5\u001b[39;00m\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m magic_number.startswith((\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCDF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\211\u001b[39;00m\u001b[33mHDF\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\032\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\utils.py:699\u001b[39m, in \u001b[36mtry_read_magic_number_from_path\u001b[39m\u001b[34m(pathlike, count)\u001b[39m\n\u001b[32m    697\u001b[39m path = os.fspath(pathlike)\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    700\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m read_magic_number_from_file(f, count)\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Step-5: Train/Val/Test Splits (ERA5 only)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "\n",
    "split_csv = PROC / \"splits.csv\"\n",
    "split_records = []\n",
    "\n",
    "def label_split(ts):\n",
    "    \"\"\"Assign split label based on year.\"\"\"\n",
    "    year = pd.to_datetime(ts).year\n",
    "    if year in TRAIN_YEARS: return \"train\"\n",
    "    if year in VAL_YEARS:   return \"val\"\n",
    "    if year in TEST_YEARS:  return \"test\"\n",
    "    return \"ignore\"\n",
    "\n",
    "# --- Apply to ERA5 Single & Pressure ---\n",
    "for folder in [ERA5_STD_SINGLE, ERA5_STD_PL]:\n",
    "    for f in tqdm(folder.glob(\"*.nc\"), desc=f\"Step-5 Splits {folder.name}\"):\n",
    "        try:\n",
    "            ds = xr.open_dataset(f)\n",
    "            split_label = label_split(ds.time)\n",
    "            split_records.append({\n",
    "                \"file\": f.name,\n",
    "                \"dataset\": folder.name,\n",
    "                \"time_start\": str(ds.time.values[0]),\n",
    "                \"time_end\": str(ds.time.values[-1]),\n",
    "                \"n_steps\": ds.dims[\"time\"],\n",
    "                \"split\": split_label,\n",
    "            })\n",
    "            ds.close()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {f}: {e}\")\n",
    "\n",
    "# Save splits summary\n",
    "pd.DataFrame(split_records).to_csv(split_csv, index=False)\n",
    "print(f\"‚úÖ Split ranges saved to {split_csv}\")\n",
    "\n",
    "# --- IMERG Handling ---\n",
    "print(\"‚ÑπÔ∏è IMERG not split at file level (too granular).\")\n",
    "print(\"   ‚Üí Use aligned years in modeling: train=2015‚Äì2016, val=2019, test=2020.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e336e9-be57-4a2b-9d7b-18edaa87ee8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63ea6b-19b7-4718-8095-b5d1b5f952d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a90156-b88a-490b-b085-67c899afa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feture Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9c729-ac39-4173-8f45-d6aa88275c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef408e86-6054-4306-b221-ae1d7f4a8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 1. Vertical Shear & Lapse Rates\n",
    "# ===========================================================\n",
    "era5_pl_files = sorted(ERA5_STD_PL.glob(\"*.nc\"))\n",
    "ds_pl = xr.open_mfdataset(era5_pl_files, combine=\"by_coords\", parallel=True)\n",
    "\n",
    "# Standardize dimension name\n",
    "if \"pressure_level\" in ds_pl.dims:\n",
    "    ds_pl = ds_pl.rename({\"pressure_level\": \"level\"})\n",
    "\n",
    "# Shear: U/V difference between 850 and 500 hPa\n",
    "ds_pl[\"shear_u_850_500\"] = ds_pl[\"u_component_of_wind\"].sel(level=850) - ds_pl[\"u_component_of_wind\"].sel(level=500)\n",
    "ds_pl[\"shear_v_850_500\"] = ds_pl[\"v_component_of_wind\"].sel(level=850) - ds_pl[\"v_component_of_wind\"].sel(level=500)\n",
    "\n",
    "# Lapse rate (temperature drop per ~3.5 km between 850‚Äì500 hPa)\n",
    "ds_pl[\"lapse_rate_850_500\"] = (ds_pl[\"temperature\"].sel(level=850) - ds_pl[\"temperature\"].sel(level=500)) / 3500.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b725bd-6758-4a82-ba97-8071cddb9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary for ERA5 Pressure-Level with IVT\n",
      "------------------------------------------------------------\n",
      "Dimensions: {'time': 186588, 'level': 4, 'lat': 53, 'lon': 67}\n",
      "Coordinates: ['level', 'lat', 'lon', 'number', 'time']\n",
      "------------------------------------------------------------\n",
      "specific_humidity              | time=186588 √ó level=4 √ó lat=53 √ó lon=67  | units: kg kg**-1\n",
      "u_component_of_wind            | time=186588 √ó level=4 √ó lat=53 √ó lon=67  | units: m s**-1\n",
      "v_component_of_wind            | time=186588 √ó level=4 √ó lat=53 √ó lon=67  | units: m s**-1\n",
      "geopotential                   | time=186588 √ó level=4 √ó lat=53 √ó lon=67  | units: m**2 s**-2\n",
      "relative_humidity              | time=186588 √ó level=4 √ó lat=53 √ó lon=67  | units: %\n",
      "temperature                    | time=186588 √ó level=4 √ó lat=53 √ó lon=67  | units: K\n",
      "shear_u_850_500                | time=186588 √ó lat=53 √ó lon=67            | units: ‚Äî\n",
      "shear_v_850_500                | time=186588 √ó lat=53 √ó lon=67            | units: ‚Äî\n",
      "lapse_rate_850_500             | time=186588 √ó lat=53 √ó lon=67            | units: ‚Äî\n",
      "IVT                            | time=186588 √ó lat=53 √ó lon=67            | units: kg m-1 s-1\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agree\\AppData\\Local\\Temp\\ipykernel_21816\\2461453635.py:92: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Step 2. Integrated Vapour Transport (IVT)\n",
    "# ===========================================================\n",
    "q = ds_pl[\"specific_humidity\"]\n",
    "u = ds_pl[\"u_component_of_wind\"]\n",
    "v = ds_pl[\"v_component_of_wind\"]\n",
    "\n",
    "# Pressure levels in Pa\n",
    "plevs = ds_pl[\"level\"] * 100.0   # <-- use 'level' instead of 'pressure_level'\n",
    "dp = np.gradient(plevs)          # (level,)\n",
    "\n",
    "# Expand dp to broadcast with (time, level, lat, lon)\n",
    "dp_xr = xr.DataArray(\n",
    "    dp,\n",
    "    dims=[\"level\"],\n",
    "    coords={\"level\": ds_pl[\"level\"]},\n",
    ")\n",
    "\n",
    "# Compute IVT components\n",
    "dx = (q * u * dp_xr).sum(dim=\"level\") / 9.81\n",
    "dy = (q * v * dp_xr).sum(dim=\"level\") / 9.81\n",
    "\n",
    "ivt = np.sqrt(dx**2 + dy**2)\n",
    "ds_pl[\"IVT\"] = ivt\n",
    "ds_pl[\"IVT\"].attrs[\"units\"] = \"kg m-1 s-1\"\n",
    "\n",
    "# -----------------------------\n",
    "# Check the result\n",
    "# -----------------------------\n",
    "print_ds_summary(ds_pl, \"ERA5 Pressure-Level with IVT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf5004-66a0-4e72-af38-852f5da565aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c48861-2f82-46b7-9517-58dc227dd6d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 35\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 55\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 35\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 55\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 35\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 55\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 35\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 55\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 35\n",
      "  result = blockwise(\n",
      "C:\\Users\\agree\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:4997: PerformanceWarning: Increasing number of chunks by factor of 55\n",
      "  result = blockwise(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\indexing.py:1688\u001b[39m, in \u001b[36mDaskIndexingAdapter._oindex_get\u001b[39m\u001b[34m(self, indexer)\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m   1690\u001b[39m     \u001b[38;5;66;03m# manual orthogonal indexing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:2038\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m   2037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2038\u001b[39m     dsk, chunks = \u001b[43mslice_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SlicingNoop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:172\u001b[39m, in \u001b[36mslice_array\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Pass down to next function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m dsk_out, bd_out = \u001b[43mslice_with_newaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m bd_out = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, bd_out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:194\u001b[39m, in \u001b[36mslice_with_newaxes\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Pass down and do work\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m dsk, blockdims2 = \u001b[43mslice_wrap_lists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_none\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where_none:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:249\u001b[39m, in \u001b[36mslice_wrap_lists\u001b[39m\u001b[34m(out_name, in_name, blockdims, index, allow_getitem_optimization)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(where_list) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDon\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yet support nd fancy indexing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Is the single list an empty list? In this case just treat it as a zero\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# length slice\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Don't yet support nd fancy indexing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 5. Anomalies (MSLP, BLH) with safe chunking\u001b[39;00m\n\u001b[32m      2\u001b[39m era5_single_files = \u001b[38;5;28msorted\u001b[39m(ERA5_STD_SINGLE.glob(\u001b[33m\"\u001b[39m\u001b[33m*.nc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ds_single = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mera5_single_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mby_coords\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m53\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m67\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# coarser chunks\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mmean_sea_level_pressure\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mboundary_layer_height\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m ds_single:\n\u001b[32m     12\u001b[39m         \u001b[38;5;66;03m# Pre-chunk to reduce warnings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\backends\\api.py:1663\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[39m\n\u001b[32m   1650\u001b[39m     combined = _nested_combine(\n\u001b[32m   1651\u001b[39m         datasets,\n\u001b[32m   1652\u001b[39m         concat_dims=concat_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1658\u001b[39m         combine_attrs=combine_attrs,\n\u001b[32m   1659\u001b[39m     )\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m combine == \u001b[33m\"\u001b[39m\u001b[33mby_coords\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1661\u001b[39m     \u001b[38;5;66;03m# Redo ordering from coordinates, ignoring how they were ordered\u001b[39;00m\n\u001b[32m   1662\u001b[39m     \u001b[38;5;66;03m# previously\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m     combined = \u001b[43mcombine_by_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1672\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1673\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombine\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is an invalid option for the keyword argument ``combine``\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\combine.py:996\u001b[39m, in \u001b[36mcombine_by_coords\u001b[39m\u001b[34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[39m\n\u001b[32m    981\u001b[39m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[32m    982\u001b[39m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[32m    983\u001b[39m     concatenated_grouped_by_data_vars = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    984\u001b[39m         _combine_single_variable_hypercube(\n\u001b[32m    985\u001b[39m             \u001b[38;5;28mtuple\u001b[39m(datasets_with_same_vars),\n\u001b[32m   (...)\u001b[39m\u001b[32m    993\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, datasets_with_same_vars \u001b[38;5;129;01min\u001b[39;00m grouped_by_vars\n\u001b[32m    994\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcatenated_grouped_by_data_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\merge.py:984\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(objects, compat, join, fill_value, combine_attrs)\u001b[39m\n\u001b[32m    981\u001b[39m         obj = obj.to_dataset()\n\u001b[32m    982\u001b[39m     dict_like_objects.append(obj)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m merge_result = \u001b[43mmerge_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdict_like_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset._construct_direct(**merge_result._asdict())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\merge.py:700\u001b[39m, in \u001b[36mmerge_core\u001b[39m\u001b[34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value, skip_align_args)\u001b[39m\n\u001b[32m    697\u001b[39m skip_align_objs = [(pos, objects.pop(pos)) \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m skip_align_args]\n\u001b[32m    699\u001b[39m coerced = coerce_pandas_values(objects)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m aligned = \u001b[43mdeep_align\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoerced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pos, obj \u001b[38;5;129;01min\u001b[39;00m skip_align_objs:\n\u001b[32m    705\u001b[39m     aligned.insert(pos, obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:1003\u001b[39m, in \u001b[36mdeep_align\u001b[39m\u001b[34m(objects, join, copy, indexes, exclude, raise_on_invalid, fill_value)\u001b[39m\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1001\u001b[39m         out.append(variables)\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m aligned = \u001b[43malign\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m position, key, aligned_obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(positions, keys, aligned, strict=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1013\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m no_key:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:939\u001b[39m, in \u001b[36malign\u001b[39m\u001b[34m(join, copy, indexes, exclude, fill_value, *objects)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[33;03mGiven any number of Dataset and/or DataArray objects, returns new\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03mobjects with aligned indexes and dimension sizes.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    929\u001b[39m \n\u001b[32m    930\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    931\u001b[39m aligner = Aligner(\n\u001b[32m    932\u001b[39m     objects,\n\u001b[32m    933\u001b[39m     join=join,\n\u001b[32m   (...)\u001b[39m\u001b[32m    937\u001b[39m     fill_value=fill_value,\n\u001b[32m    938\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m939\u001b[39m \u001b[43maligner\u001b[49m\u001b[43m.\u001b[49m\u001b[43malign\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m aligner.results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:639\u001b[39m, in \u001b[36mAligner.align\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28mself\u001b[39m.objects\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:612\u001b[39m, in \u001b[36mAligner.reindex_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreindex_all\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    613\u001b[39m         \u001b[38;5;28mself\u001b[39m._reindex_one(obj, matching_indexes, matching_index_vars)\n\u001b[32m    614\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m obj, matching_indexes, matching_index_vars \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    615\u001b[39m             \u001b[38;5;28mself\u001b[39m.objects,\n\u001b[32m    616\u001b[39m             \u001b[38;5;28mself\u001b[39m.objects_matching_indexes,\n\u001b[32m    617\u001b[39m             \u001b[38;5;28mself\u001b[39m.objects_matching_index_vars,\n\u001b[32m    618\u001b[39m             strict=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    619\u001b[39m         )\n\u001b[32m    620\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:613\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreindex_all\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    612\u001b[39m     \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatching_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatching_index_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m obj, matching_indexes, matching_index_vars \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    615\u001b[39m             \u001b[38;5;28mself\u001b[39m.objects,\n\u001b[32m    616\u001b[39m             \u001b[38;5;28mself\u001b[39m.objects_matching_indexes,\n\u001b[32m    617\u001b[39m             \u001b[38;5;28mself\u001b[39m.objects_matching_index_vars,\n\u001b[32m    618\u001b[39m             strict=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    619\u001b[39m         )\n\u001b[32m    620\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:601\u001b[39m, in \u001b[36mAligner._reindex_one\u001b[39m\u001b[34m(self, obj, matching_indexes, matching_index_vars)\u001b[39m\n\u001b[32m    596\u001b[39m new_indexes, new_variables = \u001b[38;5;28mself\u001b[39m._get_indexes_and_vars(\n\u001b[32m    597\u001b[39m     obj, matching_indexes, matching_index_vars\n\u001b[32m    598\u001b[39m )\n\u001b[32m    599\u001b[39m dim_pos_indexers = \u001b[38;5;28mself\u001b[39m._get_dim_pos_indexers(matching_indexes)\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reindex_callback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdim_pos_indexers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexclude_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexclude_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\dataset.py:3298\u001b[39m, in \u001b[36mDataset._reindex_callback\u001b[39m\u001b[34m(self, aligner, dim_pos_indexers, variables, indexes, fill_value, exclude_dims, exclude_vars)\u001b[39m\n\u001b[32m   3292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3293\u001b[39m     to_reindex = {\n\u001b[32m   3294\u001b[39m         k: v\n\u001b[32m   3295\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.variables.items()\n\u001b[32m   3296\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m variables \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude_vars\n\u001b[32m   3297\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m3298\u001b[39m     reindexed_vars = \u001b[43malignment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto_reindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdim_pos_indexers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43maligner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3303\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maligner\u001b[49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3305\u001b[39m     new_variables.update(reindexed_vars)\n\u001b[32m   3306\u001b[39m     new_coord_names = \u001b[38;5;28mself\u001b[39m._coord_names | \u001b[38;5;28mset\u001b[39m(new_indexes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\structure\\alignment.py:83\u001b[39m, in \u001b[36mreindex_variables\u001b[39m\u001b[34m(variables, dim_pos_indexers, copy, fill_value, sparse)\u001b[39m\n\u001b[32m     80\u001b[39m needs_masking = \u001b[38;5;28many\u001b[39m(d \u001b[38;5;129;01min\u001b[39;00m masked_dims \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m var.dims)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m needs_masking:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     new_var = \u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getitem_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindxr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_full_slice(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m indxr):\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# no reindexing necessary\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# here we need to manually deal with copying data, since\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# we neither created a new ndarray nor used fancy indexing\u001b[39;00m\n\u001b[32m     88\u001b[39m     new_var = var.copy(deep=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\variable.py:826\u001b[39m, in \u001b[36mVariable._getitem_with_mask\u001b[39m\u001b[34m(self, key, fill_value)\u001b[39m\n\u001b[32m    823\u001b[39m     actual_indexer = indexer\n\u001b[32m    825\u001b[39m indexable = as_indexable(\u001b[38;5;28mself\u001b[39m._data)\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m data = \u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_indexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m mask = indexing.create_mask(indexer, \u001b[38;5;28mself\u001b[39m.shape, data)\n\u001b[32m    829\u001b[39m \u001b[38;5;66;03m# we need to invert the mask in order to pass data first. This helps\u001b[39;00m\n\u001b[32m    830\u001b[39m \u001b[38;5;66;03m# pint to choose the correct unit\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[38;5;66;03m# TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\indexing.py:1036\u001b[39m, in \u001b[36mapply_indexer\u001b[39m\u001b[34m(indexable, indexer)\u001b[39m\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indexable.vindex[indexer]\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, OuterIndexer):\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexable\u001b[49m\u001b[43m.\u001b[49m\u001b[43moindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indexable[indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\indexing.py:372\u001b[39m, in \u001b[36mIndexCallable.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\xarray\\core\\indexing.py:1693\u001b[39m, in \u001b[36mDaskIndexingAdapter._oindex_get\u001b[39m\u001b[34m(self, indexer)\u001b[39m\n\u001b[32m   1691\u001b[39m value = \u001b[38;5;28mself\u001b[39m.array\n\u001b[32m   1692\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, subkey \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(key))):\n\u001b[32m-> \u001b[39m\u001b[32m1693\u001b[39m     value = \u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\core.py:2038\u001b[39m, in \u001b[36mArray.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m   2036\u001b[39m out = \u001b[33m\"\u001b[39m\u001b[33mgetitem-\u001b[39m\u001b[33m\"\u001b[39m + tokenize(\u001b[38;5;28mself\u001b[39m, index2)\n\u001b[32m   2037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2038\u001b[39m     dsk, chunks = \u001b[43mslice_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SlicingNoop:\n\u001b[32m   2040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:172\u001b[39m, in \u001b[36mslice_array\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    169\u001b[39m index += (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),) * missing\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Pass down to next function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m dsk_out, bd_out = \u001b[43mslice_with_newaxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m bd_out = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m, bd_out))\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dsk_out, bd_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:194\u001b[39m, in \u001b[36mslice_with_newaxes\u001b[39m\u001b[34m(out_name, in_name, blockdims, index)\u001b[39m\n\u001b[32m    191\u001b[39m         where_none[i] -= n\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Pass down and do work\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m dsk, blockdims2 = \u001b[43mslice_wrap_lists\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_none\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where_none:\n\u001b[32m    199\u001b[39m     expand = expander(where_none)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:271\u001b[39m, in \u001b[36mslice_wrap_lists\u001b[39m\u001b[34m(out_name, in_name, blockdims, index, allow_getitem_optimization)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_arraylike(i) \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m index):\n\u001b[32m    270\u001b[39m     axis = where_list[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     blockdims2, dsk3 = \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwhere_list\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Mixed case. Both slices/integers and lists. slice/integer then take\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Do first pass without lists\u001b[39;00m\n\u001b[32m    277\u001b[39m     tmp = \u001b[33m\"\u001b[39m\u001b[33mslice-\u001b[39m\u001b[33m\"\u001b[39m + tokenize((out_name, in_name, blockdims, index))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\slicing.py:630\u001b[39m, in \u001b[36mtake\u001b[39m\u001b[34m(outname, inname, chunks, index, axis)\u001b[39m\n\u001b[32m    623\u001b[39m         indexer.append(index[i : i + average_chunk_size].tolist())\n\u001b[32m    625\u001b[39m     token = (\n\u001b[32m    626\u001b[39m         outname.split(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m    627\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outname\n\u001b[32m    628\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m tokenize(outname, chunks, index, axis)\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     chunks, graph = \u001b[43m_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks, graph\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunks[axis]) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\conda_envs\\rain311\\Lib\\site-packages\\dask\\array\\_shuffle.py:276\u001b[39m, in \u001b[36m_shuffle\u001b[39m\u001b[34m(chunks, indexer, axis, in_name, out_name, token)\u001b[39m\n\u001b[32m    271\u001b[39m         intermediates[taker_key] = DataNode(\n\u001b[32m    272\u001b[39m             taker_key, (\u001b[32m1\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(this_slice))\n\u001b[32m    273\u001b[39m         )\n\u001b[32m    274\u001b[39m         taker_cache[c] = taker_key\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     intermediates[name] = \u001b[43mTask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTaskRef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTaskRef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     merge_keys.append(name)\n\u001b[32m    281\u001b[39m merge_suffix = convert_key(chunk_tuple, new_chunk_idx, axis)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 3. Anomalies (MSLP, BLH) with safe chunking\n",
    "era5_single_files = sorted(ERA5_STD_SINGLE.glob(\"*.nc\"))\n",
    "ds_single = xr.open_mfdataset(\n",
    "    era5_single_files,\n",
    "    combine=\"by_coords\",\n",
    "    parallel=True,\n",
    "    chunks={\"time\": 5000, \"latitude\": 53, \"longitude\": 67}  # coarser chunks\n",
    ")\n",
    "\n",
    "for v in [\"mean_sea_level_pressure\", \"boundary_layer_height\"]:\n",
    "    if v in ds_single:\n",
    "        # Pre-chunk to reduce warnings\n",
    "        ds_var = ds_single[v].chunk({\"time\": -1})\n",
    "        \n",
    "        # Monthly climatology\n",
    "        clim = ds_var.groupby(\"time.month\").mean(\"time\")\n",
    "        \n",
    "        # Subtract climatology\n",
    "        ds_single[f\"{v}_anom\"] = ds_var.groupby(\"time.month\") - clim\n",
    "\n",
    "print(\"‚úÖ Anomalies computed with controlled chunking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c54b9-8b68-49ec-a026-7b2bc22fdb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec22a50-996e-4b65-b3c1-fce7fe0b841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 4. IMERG Persistence (rolling sums)\n",
    "# ===========================================================\n",
    "imerg_files = sorted(IMERG_STD.glob(\"*.nc\"))\n",
    "ds_imerg = xr.open_mfdataset(imerg_files, combine=\"by_coords\", parallel=True)\n",
    "\n",
    "if \"precipitation_mm30\" in ds_imerg:\n",
    "    ds_imerg[\"imerg_persist_1h\"] = ds_imerg[\"precipitation_mm30\"].rolling(time=2, min_periods=1).sum()\n",
    "    ds_imerg[\"imerg_persist_3h\"] = ds_imerg[\"precipitation_mm30\"].rolling(time=6, min_periods=1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6840f2-299b-415a-9b87-b785d6ccb6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb906f4-04a8-4356-81f3-0c511967d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 5. Moisture Flux Convergence\n",
    "# ===========================================================\n",
    "R_earth = 6.371e6\n",
    "lat = np.deg2rad(ds_pl[\"latitude\"])\n",
    "lon = np.deg2rad(ds_pl[\"longitude\"])\n",
    "\n",
    "uq = ds_pl[\"specific_humidity\"].sel(level=850) * ds_pl[\"u_component_of_wind\"].sel(level=850)\n",
    "vq = ds_pl[\"specific_humidity\"].sel(level=850) * ds_pl[\"v_component_of_wind\"].sel(level=850)\n",
    "\n",
    "dlon = np.gradient(lon)\n",
    "dlat = np.gradient(lat)\n",
    "\n",
    "uq_dx = (uq.diff(\"longitude\") / dlon.mean()) / (R_earth * np.cos(lat.mean()))\n",
    "vq_dy = (vq.diff(\"latitude\") / dlat.mean()) / R_earth\n",
    "\n",
    "div_q = uq_dx.pad({\"longitude\": (0,1)}) + vq_dy.pad({\"latitude\": (0,1)})\n",
    "ds_pl[\"moisture_flux_conv\"] = -div_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420032e-a87e-4fc8-8df1-c5787708efe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83365ead-48e5-44da-a471-fe31ca6f25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Step 6. CAPE Proxy\n",
    "# ===========================================================\n",
    "T = ds_pl[\"temperature\"]\n",
    "q = ds_pl[\"specific_humidity\"]\n",
    "\n",
    "# Potential temperature Œ∏ = T * (1000/p)**0.286\n",
    "theta = T * (1000.0 / ds_pl[\"level\"]) ** 0.286\n",
    "theta_sfc = theta.sel(level=850)\n",
    "theta_mid = theta.sel(level=500)\n",
    "\n",
    "delta_theta = theta_sfc - theta_mid\n",
    "cape_proxy = delta_theta * q.sel(level=850)\n",
    "\n",
    "ds_pl[\"CAPE_proxy\"] = cape_proxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ad461-6e92-49f3-895f-183a89e790d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rain311)",
   "language": "python",
   "name": "rain311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
